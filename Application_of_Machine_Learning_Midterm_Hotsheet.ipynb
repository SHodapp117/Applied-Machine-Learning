{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOwKohX7vHvH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print these to make sure you understand what is being generated.\n",
        "A = np.array([1, 2, 3])\n",
        "B = np.arange(1, 13).reshape(3, 4)\n",
        "C = np.ones((2, 3))\n",
        "D = np.eye(3)\n",
        "\n",
        "\n",
        "print(A)\n",
        "print(B)\n",
        "\n",
        "#print(C)\n",
        "#print(D)"
      ],
      "metadata": {
        "id": "bL5l2XwGvO3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the following computations using numpy functions and print the results. Note that the * operator implies matrix multiplication (typically denoted using dot notation in linear algebra) -- make sure the dimensions align!\n",
        "\n",
        "2A + 1\n",
        "Sum the rows of B (Hint: pass axis=1 to the cooresponding sum function)\n",
        "Sum the columns of B (Hint: pass axis=0 to the cooresponding sum function)\n",
        "Number of elements of B greater than 5\n",
        "C + C\n",
        "A * B"
      ],
      "metadata": {
        "id": "pQnBCbMjvUSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution to Question 1:\n",
        "a = 2 * A + 1\n",
        "print(a)\n",
        "\n",
        "#Solution to Question 2:\n",
        "b = np.sum(B,axis=1)\n",
        "print(b)\n",
        "\n",
        "#Solution to Question 3:\n",
        "bb = np.sum(B,axis=0)\n",
        "print(bb)\n",
        "\n",
        "\n",
        "#Solution to Question 4:\n",
        "bbb = B[B > 5]\n",
        "print(bbb.size)\n",
        "#print(bbb)\n",
        "\n",
        "\n",
        "#Solution to Question 5:\n",
        "c = C + C\n",
        "print(c)\n",
        "\n",
        "#Solution to Question 6:\n",
        "reshaped_b = B.reshape(4,3)\n",
        "solution = reshaped_b * A\n",
        "print(solution)"
      ],
      "metadata": {
        "id": "zYqPZDvHvVCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IJwK_AFlvXsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data for Supervised Learning\n",
        "Supervised learning is all about learning to make predictions: given an input  x  (e.g. home square footage), can we produce an output  y^  (e.g. estimated value) as close to the actual observed output  y  (e.g. sale price) as possible. Note that the \"hat\" above  y  is used to denote an estimated or predicted value.\n",
        "\n",
        "Let's start by generating some artificial data. We'll create a vector of inputs,  X , and a corresponding vector of target outputs  Y . In general, we'll refer to invidual examples with a lowercase ( x ), and a vector or matrix containing multiple examples with a capital ( X )."
      ],
      "metadata": {
        "id": "WKIiOym7vaEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_1d_data(num_examples=10, w=2, b=1, random_scale=1):\n",
        "  \"\"\"Create X, Y data with a linear relationship with added noise.\n",
        "\n",
        "  Args:\n",
        "    num_examples: number of examples to generate\n",
        "    w: desired slope\n",
        "    b: desired intercept\n",
        "    random_scale: add uniform noise between -random_scale and +random_scale\n",
        "\n",
        "  Returns:\n",
        "    X and Y with shape (num_examples)\n",
        "  \"\"\"\n",
        "  X = np.arange(num_examples)\n",
        "  np.random.seed(4)  # consistent random number generation\n",
        "  deltas = np.random.uniform(low=-random_scale, high=random_scale, size=X.shape)\n",
        "  Y = b + deltas + w * X\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "1REsvumSvatF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some artificial data using create_1d_data.\n",
        "X, Y = create_1d_data()\n",
        "plt.scatter(X, Y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vz7pf0Ravdgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A model is a function that takes an input  x  and produces a prediction  y^ .\n",
        "\n",
        "Let's consider two possible models for this data:\n",
        "\n",
        "M1(x)=x+5\n",
        "M2(x)=2x+1\n",
        "Compute the predictions of models  M1  and  M2  for the values in  X . These predictions should be vectors of the same shape as  Y . Then plot the prediction lines of these two models overlayed on the \"observed\" data  (X,Y) . Use plt.plot() to draw the lines. Note: you will generate only one plot. Make sure to include axes, titles and legend."
      ],
      "metadata": {
        "id": "78bBGo_SviYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = X + 5\n",
        "m2 = 2 * X + 1\n",
        "\n",
        "#Put in the baseline\n",
        "plt.scatter(X, Y)\n",
        "\n",
        "# Plot the data\n",
        "plt.plot(X, m1, label=\"Model_1\",color='Black')\n",
        "plt.plot(X, m2, label=\"Model_2\", color='Green')\n",
        "\n",
        "# Add a legend & Title & Axis\n",
        "plt.title(\"Comparing Model 1 & Model 2\")\n",
        "plt.xlabel(\"X-Axis\")\n",
        "plt.ylabel(\"Y-Axis\")\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xZSASM8vvi6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How good are our models? Intuitively, the better the model, the more closely it fits the data we have. That is, for each  x , we'll compare  y , the true value, with  y^ , the predicted value. This comparison is often called the loss or the error. One common such comparison is squared error:  (y−y^)2 . Averaging over all our data points, we get the mean squared error:\n",
        "\n",
        "MSE=1n∑yi∈Y(yi−y^i)2"
      ],
      "metadata": {
        "id": "d58CFs6Kvnoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(true_values, predicted_values):\n",
        "  \"\"\"Return the MSE between true_values and predicted values\n",
        "  Args:\n",
        "    y_true: The given labels.\n",
        "    y_pred: The predicted labels.\n",
        "\n",
        "  Returns:\n",
        "    The mean squared error.\n",
        "  \"\"\"\n",
        "  # Compute the squared error for each element.\n",
        "  squared_error = (true_values - predicted_values)**2\n",
        "\n",
        "  # Return the mean of the squared errors.\n",
        "  return np.mean(squared_error)"
      ],
      "metadata": {
        "id": "PMJIxd1WvqNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('MSE for M1:', MSE(Y, m1))\n",
        "print ('MSE for M2:', MSE(Y, m2))"
      ],
      "metadata": {
        "id": "qb1DeqjEvrGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data  (X,Y)  represents just a sample of all possible input-output pairs we might care about. A model will be useful to the extent we can apply it to new inputs. Consider the more complex model below, which appears to produce a much smaller mean squared error."
      ],
      "metadata": {
        "id": "XIzH9SyVvvCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit an 8-th degree polynomial to (X, Y). See np.polyfit for details.\n",
        "polynomial_model_coefficients = np.polyfit(X, Y, deg=8)\n",
        "polynomial_model = np.poly1d(polynomial_model_coefficients)\n",
        "M3 = polynomial_model(X)\n",
        "fig = plt.scatter(X, Y)\n",
        "plt.plot(X, M3, '-k')\n",
        "print ('MSE for M3:', MSE(Y, M3))"
      ],
      "metadata": {
        "id": "_zS0zAAkvsjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review\n",
        "In Supervised Machine Learning, we must start with data in the form  (X,Y)  where  X  are the inputs and  Y  are the output labels.\n",
        "A model is a function that maps an input  x  to an output  y . The model's output is referred to as a prediction, denoted by  y^ .\n",
        "We evaluate predictions by comparing them to the true labels. This measurement is called a loss or error. For real-valued data, mean squared error is a common metric.\n",
        "A model is only as good as its ability to generalize to new examples."
      ],
      "metadata": {
        "id": "w0icWOuwv04N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the results of the following calculations:\n",
        "\n",
        "Add a column of 1s to  X .\n",
        "Use matrix multiplication (np.dot) to create  M1  and  M2  (from above) to produce vectors of predictions.\n",
        "Print the shapes of the predictions to validate that they have the same shape as  Y ."
      ],
      "metadata": {
        "id": "e4dWrOgHDILi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column of 1s to X by using np.c_ to concatenate with the current values.\n",
        "X_with_1s = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "# Define the functions M1(x) and M2(x)\n",
        "def M1(x):\n",
        "    return np.dot(x, np.array([5, 1]))  # Corresponds to x + 5\n",
        "\n",
        "def M2(x):\n",
        "    return np.dot(x, np.array([2, 1]))  # Corresponds to 2x + 1\n",
        "\n",
        "# Calculate predictions using M1 and M2\n",
        "predictions_M1 = M1(X_with_1s)\n",
        "predictions_M2 = M2(X_with_1s)\n",
        "\n",
        "\n",
        "# Print the shapes of the predictions to validate that they have the same shape as Y.\n",
        "print(\"Shape of predictions for M1:\", predictions_M1.shape)\n",
        "print(\"Shape of predictions for M2:\", predictions_M2.shape)\n",
        "\n",
        "# Assuming Y is already defined, you can also print its shape for comparison.\n",
        "print(\"Shape of Y:\", Y.shape)\n"
      ],
      "metadata": {
        "id": "MpYL5r-sDJBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we'll demonstrate gradient descent for linear regression to learn the weight vector  W . We'll use the more specific notation  hW(x)  since we want to specify that  h  is parameterized by  W . As above, we'll assume that  x0=1  so we can write  h  as a sum or a matrix product:\n",
        "\n",
        "hW(x)=∑i=0n−1wixi=xWT\n",
        "\n",
        "In the derivation that follows, we'll use summations, but in the code below, we'll use matrix computations.\n",
        "\n",
        "In linear regression, we compute the loss,  J(W)  from the mean squared difference between predictions  hW(x)  and targets  y . In the following equation, we average the loss over each of the  m  training examples.\n",
        "\n",
        "J(W)=12m∑i=0m−1(hW(x(i))−y(i))2\n",
        "\n",
        "Dividing by  2  simplifies the formula of the gradient, since it cancels out the constant  2  from by the derivative of the squared term (see below). Remember that the gradient is a vector of partial derivatives for each  wj  (holding the other elements of  w  constant). The gradient points in direction of steepest ascent for the loss function  J .\n",
        "\n",
        "Here we derive the parameter update rule by computing the gradient of the loss function. We need a derivative for each feature in  x , so we'll show how to compute the derivative with respect to  wj . For simplicity, let's assume we have only one training example ( m=1 ):\n",
        "\n",
        "∂∂wjJ(W)=∂∂wj12(hW(x)−y)2=2⋅12(hW(x)−y)⋅∂∂wj(hW(x)−y)=(hW(x)−y)∂∂wj(∑i=0n−1wixi−y)=(hW(x)−y)xj(1)(2)(3)(4)\n",
        "\n",
        "The derivation has 2 key steps:\n",
        "\n",
        "(1) Apply the chain rule (step 1 -> 2).\n",
        "\n",
        "(2) The derivative with respect to  wj  of  hW(x)  is only non-zero for  wjxj . For this component, the derivative is  xj  since the feature value is treated as a constant (step 3 -> 4).\n",
        "\n",
        "Ok, that's it. We can now implement gradient descent for linear regression. The only difference in the code below is that it computes the loss as an average over all training examples (rather than just a single example)."
      ],
      "metadata": {
        "id": "TzpqOLKRDSdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the NotImplemented parts of the gradient descent function below. There are detailed comments to help guide you. Note that this function uses vectors and matrices so you'll want to use numpy functions like np.dot to multiply them, for example."
      ],
      "metadata": {
        "id": "MgTI6Np4DV-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(inputs, outputs, learning_rate, num_epochs):\n",
        "    \"\"\"Apply the gradient descent algorithm to learn linear regression.\n",
        "\n",
        "    Args:\n",
        "        inputs: A 2-D array where each column is an input feature, and each\n",
        "                row is a training example.\n",
        "        outputs: A 1-D array containing the real-valued\n",
        "                 label corresponding to the input data in the same row.\n",
        "        learning_rate: The learning rate to use for updates.\n",
        "        num_epochs: The number of passes through the full training data.\n",
        "\n",
        "    Returns:\n",
        "        weights: A 2-D array with the learned weights after each training epoch.\n",
        "        losses: A 1-D array with the loss after each epoch.\n",
        "    \"\"\"\n",
        "    # m = number of examples, n = number of features\n",
        "    m, n = inputs.shape\n",
        "\n",
        "    # We'll use a vector of size n to store the learned weights and initialize\n",
        "    # all weights to 1.\n",
        "    W = np.ones(n)\n",
        "\n",
        "    # Keep track of the training loss and weights after each step.\n",
        "    losses = []\n",
        "    weights = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Append the old weights to the weights list to keep track of them.\n",
        "        weights.append(W)\n",
        "\n",
        "        # Evaluate the current predictions for the training examples given\n",
        "        # the current estimate of W.\n",
        "        predictions = np.dot(inputs, W)\n",
        "\n",
        "        # Find the difference between the predictions and the actual target\n",
        "        # values.\n",
        "        diff = predictions - outputs\n",
        "\n",
        "        # In standard linear regression, we want to minimize the sum of squared\n",
        "        # differences. Compute the mean squared error loss.\n",
        "        loss = np.sum(diff**2) / m\n",
        "\n",
        "        # Append the loss to the losses list to keep track of it.\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Compute the gradient\n",
        "        # Gradient of the mean squared error loss with respect to weights is:\n",
        "        gradient = 2 * np.dot(inputs.T, diff) / m\n",
        "\n",
        "        # Update weights, scaling the gradient by the learning rate.\n",
        "        W = W - learning_rate * gradient\n",
        "\n",
        "    return np.array(weights), np.array(losses)"
      ],
      "metadata": {
        "id": "_hkQaafSDVA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try running gradient descent with our artificial data and print out the results. Note that we're passing the version of the input data with a column of  1s  so that we learn an intercept (also called a bias). We can also try learning without the intercept.\n",
        "\n",
        "Note: if your implementation of gradient descent is correct, you should get a loss of ~0.409 after 5 epochs (with a bias parameter)."
      ],
      "metadata": {
        "id": "i9qkh6c8DbXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Running gradient descent...')\n",
        "weights, losses = gradient_descent(X_with_1s, Y, learning_rate=.02,\n",
        "                                   num_epochs=5)\n",
        "for W, loss in zip(weights, losses):\n",
        "  print(loss, W)\n",
        "\n",
        "print('\\nRunning gradient descent without biases...')\n",
        "# Make sure we're providing an input with the right 2-D shape.\n",
        "X_without_1s = np.expand_dims(X, axis=0).T\n",
        "weights_without_bias, losses_without_bias = gradient_descent(X_without_1s, Y,\n",
        "                                                             .02, num_epochs=5)\n",
        "for W, loss in zip(weights_without_bias, losses_without_bias):\n",
        "  print(loss, W)"
      ],
      "metadata": {
        "id": "qDdyQwj_Dc4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a function that lets us visualize the progress of gradient descent during training. Our gradient descent function already provides intermediate weight vectors and losses after each epoch, so we just need to plot these."
      ],
      "metadata": {
        "id": "t9z2bUccDfUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning(inputs, outputs, weights, losses):\n",
        "  \"\"\"Plot predictions and losses after each training epoch.\n",
        "\n",
        "  Args:\n",
        "    inputs: A 2-D array where each column is an input feature and each\n",
        "            row is a training example.\n",
        "    outputs: A 1-D array containing the real-valued\n",
        "             label corresponding to the input data in the same row.\n",
        "    weights: A 2-D array with the learned weights after each training epoch.\n",
        "    losses: A 1-D array with the loss after each epoch.\n",
        "  \"\"\"\n",
        "  # Create a figure.\n",
        "  plt.figure(1, figsize=[10,4])\n",
        "\n",
        "  # The first subplot will contain the predictions. Start by plotting the\n",
        "  # outputs (Y).\n",
        "  plt.subplot(121)\n",
        "  plt.title('Model Predictions')\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.xticks(inputs[:,1])\n",
        "  plt.scatter(inputs[:,1], outputs, color='black', label='Y')\n",
        "\n",
        "  # For each epoch, retrieve the estimated weights W, compute predictions, and\n",
        "  # plot the resulting line.\n",
        "  num_epochs = len(weights)\n",
        "  for i in range(num_epochs):\n",
        "    W = weights[i]\n",
        "    predictions = np.dot(inputs, W.T)\n",
        "    plt.plot(inputs[:,1], predictions, label='Epoch %d' %i)\n",
        "  plt.legend()\n",
        "\n",
        "  # The second subplot will contain the losses.\n",
        "  plt.subplot(122)\n",
        "  plt.title('Loss function over epochs')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.xticks(range(num_epochs))\n",
        "  plt.plot(range(num_epochs), losses, marker='o', color='black',\n",
        "           linestyle='dashed')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "U2aM802IDg8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run gradient descent with learning_rate=0.01 and num_epochs=7\n",
        "new_learning_rate = 0.01\n",
        "new_num_epochs = 7\n",
        "\n",
        "new_weights, new_losses = gradient_descent(X_with_1s, Y, learning_rate=new_learning_rate, num_epochs=new_num_epochs)\n",
        "\n",
        "# Plot learning progress using new_weights and new_losses\n",
        "plot_learning(X_with_1s, Y, new_weights, new_losses)"
      ],
      "metadata": {
        "id": "dgR9bOlIDkGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We store our data in arrays where each row is an input example  x(i)  and each column is a feature. Training example  x(i)  corresponds to training label  y(i) .\n",
        "Gradient descent is an optimization process that minimizes loss  J(W)  where  W  is a set of parameters (or weights). The loss measures the difference between predictions  Y^  using the current values of  W  and the target labels  Y , and gradient descent updates  W  by taking a step in the direction of the loss gradient.\n",
        "Each pass over the training data by the gradient descent algorithm is called an epoch. The algorithm has no specific stopping point, but we often choose to stop when the parameter values have converged, that is, the change in values in the next step are less than some small  ϵ ."
      ],
      "metadata": {
        "id": "ahVOomQADmYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "elaTcaVgDxrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style=\"darkgrid\")  # default style\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "2AXN2euSD0ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the names for the feature columns since the CSV file with the data\n",
        "# does not have a header row.\n",
        "cols = ['symboling', 'losses', 'make', 'fuel-type', 'aspiration', 'num-doors',\n",
        "        'body-style', 'drive-wheels', 'engine-location', 'wheel-base',\n",
        "        'length', 'width', 'height', 'weight', 'engine-type', 'num-cylinders',\n",
        "        'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio',\n",
        "        'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
        "\n",
        "# Load the data from a CSV file into a pandas dataframe. Remember that each row\n",
        "# is an example and each column in a feature.\n",
        "car_data = pd.read_csv(\n",
        "    'https://storage.googleapis.com/ml_universities/cars_dataset/cars_data.csv',\n",
        "    sep=',', names=cols, header=None, encoding='latin-1')\n",
        "\n",
        "# Display applies built-in formatting for nicer printing, if available.\n",
        "display(car_data)"
      ],
      "metadata": {
        "id": "cfYCR9GgD0J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we'll be using SGD (Stochastic Gradient Descent) for training, it is important that each batch is a random sample of the data so that the gradient computed is representative. Note that the original data (above) appears sorted by make in alphabetic order."
      ],
      "metadata": {
        "id": "7a8DDCpFD5W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to shuffle the order of the rows without touching the columns.\n",
        "# First, we get a list of indices corresponding to the rows.\n",
        "indices = np.arange(car_data.shape[0])\n",
        "print('indices:', indices, '\\n')\n",
        "\n",
        "# Next, we shuffle the indices using np.random.permutation but set a random seed\n",
        "# so that everyone gets the same results each time.\n",
        "np.random.seed(0)\n",
        "shuffled_indices = np.random.permutation(indices)\n",
        "print('shuffled indices:', shuffled_indices, '\\n')\n",
        "\n",
        "# Finally, we use dataframe.reindex to change the ordering of the original\n",
        "# dataframe.\n",
        "car_data = car_data.reindex(shuffled_indices)\n",
        "display(car_data)\n",
        "\n",
        "# Note that this could be done in one fancy line:\n",
        "# car_data = car_data.reindex(np.random.permutation(car_data.shape[0]))"
      ],
      "metadata": {
        "id": "FpsN0cpGD0S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection\n",
        "To keep things simple, we will keep just a few of the 26 columns. Since the values come as strings, we need to convert them to floats. Also, we remove examples (rows) that have some missing value(s) of the columns we care about. Note that in general, there are various ways to deal with missing features, and this strategy of dropping examples with any missing feature is not ideal."
      ],
      "metadata": {
        "id": "7BMB33Y_D8BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a subset of columns (these are all numeric).\n",
        "columns = ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
        "car_data = car_data[columns]\n",
        "\n",
        "# Convert strings to numeric values, coercing missing values to nan.\n",
        "for column in columns:\n",
        "  car_data[column] = pd.to_numeric(car_data[column], errors='coerce')\n",
        "\n",
        "# The dropna function drops rows with missing value(s) by default.\n",
        "car_data = car_data.dropna()\n",
        "\n",
        "# This leaves us with 199 examples.\n",
        "display(car_data)"
      ],
      "metadata": {
        "id": "e1bh9Q5iD-Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've shuffled the order, we can split into portions for train and test easily. We'll try to avoid looking at the test data.\n",
        "\n",
        "We're going to train models that predict price from the other columns, so we'll create separate variables for input and output data."
      ],
      "metadata": {
        "id": "GbqXrVomEAnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use these input features.\n",
        "features = ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg']\n",
        "\n",
        "# Use a ~80/20 train/test split.\n",
        "car_train = car_data[:160]\n",
        "car_test = car_data[160:]\n",
        "\n",
        "# Create separate variables for features (inputs) and labels (outputs).\n",
        "# We will be using these in the cells below.\n",
        "car_train_features = car_train[features]\n",
        "car_test_features = car_test[features]\n",
        "car_train_labels = car_train['price']\n",
        "car_test_labels = car_test['price']\n",
        "\n",
        "# Confirm the data shapes are as expected.\n",
        "print('train data shape:', car_train_features.shape)\n",
        "print('train labels shape:', car_train_labels.shape)\n",
        "print('test data shape:', car_test_features.shape)\n",
        "print('test labels shape:', car_test_labels.shape)"
      ],
      "metadata": {
        "id": "K4SszXeMECLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have test data, we can evaluate a baseline. We'll use the average price of cars in the training set as our baseline model -- that is, the baseline always predicts the average price regardless of the input. And, instead of MSE, let's use RMSE (root mean squared error) -- that is, just take the square root of the MSE -- as our evaluation metric. Print out the following:\n",
        "\n",
        "Implement this baseline.\n",
        "Compute the RMSE of the baseline on the train data.\n",
        "Compute the RMSE of the baseline on the test data.\n",
        "Is the test RMSE larger or smaller than the train RMSE? Explain whether this is what you'd expect."
      ],
      "metadata": {
        "id": "aY8jWL6QEEyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the baseline predictor:\n",
        "def baseline_model():\n",
        "  return np.mean(car_train['price'])\n",
        "\n",
        "features = baseline_model()\n",
        "\n",
        "#Compute the RMSE:\n",
        "def RMSE(feature, labels):\n",
        "  \"\"\"\n",
        "  Calculates the root mean squared error between two arrays.\n",
        "  Args:\n",
        "    feature: The ground truth values.\n",
        "    labels: The predicted values.\n",
        "  Returns:\n",
        "    The root mean squared error.\n",
        "  \"\"\"\n",
        "  # Calculate the squared error.\n",
        "  squared_error = (feature - labels)**2\n",
        "  # Calculate the mean of the squared error.\n",
        "  mean_squared_error = np.mean(squared_error)\n",
        "  # Calculate the square root of the mean squared error.\n",
        "  root_mean_squared_error = np.sqrt(mean_squared_error)\n",
        "  return root_mean_squared_error\n",
        "\n",
        "# compute the RMSE of the baseline of the trainning data\n",
        "RMSE_Trainning_Baseline = RMSE(features , car_train_labels)\n",
        "\n",
        "# compute the RMSE of the baseline of the test data\n",
        "RMSE_Test_Baseline = RMSE(features , car_test_labels)\n",
        "\n",
        "\n",
        "#Printing the outputs:\n",
        "print(\"RMSE_Trainning_Baseline\")\n",
        "print('%.2f' % RMSE_Trainning_Baseline)\n",
        "print('---------------------------------------')\n",
        "print(\"RMSE_Testing_Baseline\")\n",
        "print('%.2f' % RMSE_Test_Baseline)\n"
      ],
      "metadata": {
        "id": "PkUOu9FiEGiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's hard to stare at a matrix of 160x5 numbers (the shape of our training data) and know what to make of it. Plotting feature histograms is a good way to start building intuition about the data. This gives us a sense of the distribution of each feature, but not how the features relate to each other.\n",
        "\n",
        "We can also use the describe function to look at some aggregate statistics for the DataFrame."
      ],
      "metadata": {
        "id": "EkZCW07ZEJln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 3))\n",
        "for i in range(len(columns)):\n",
        "  plt.subplot(1, 5, i+1)\n",
        "  plt.hist(np.array(car_train[columns[i]]))\n",
        "  plt.title(columns[i])\n",
        "plt.show()\n",
        "\n",
        "display(car_train.describe())"
      ],
      "metadata": {
        "id": "5FyvpjM4ELP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using pandas corr() to print all the pairwise correlation coefficients for the columns (use the training data only). See also the Wikipedia page on correlation for more background.\n",
        "\n",
        "Then answer the following questions:\n",
        "\n",
        "It appears that higher-priced cars have higher or lower fuel efficiency?\n",
        "Which two features are likely to be most redundant?\n",
        "Which feature is likely to be least useful for predicting price?\n",
        "Extra (ungraded): try using sns.pairplot to examine each pair of features."
      ],
      "metadata": {
        "id": "-dOO46gRENWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#making pairwise corr coefficents\n",
        "# Calculate pairwise correlation coefficients for the training data\n",
        "correlation_matrix = car_train.corr()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(correlation_matrix)\n",
        "#extra work\n",
        "# Combine the features and labels into one DataFrame\n",
        "car_train_combined = car_train.copy()\n",
        "car_train_combined['price'] = car_train_labels\n",
        "\n",
        "# Create a pairplot\n",
        "sns.pairplot(car_train_combined)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YBllFRzAEPOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Written answer:\n",
        "\n",
        "Higher-Priced Cars and Fuel Efficiency: Higher-priced cars tend to have lower fuel efficiency. This can be inferred from the negative correlations between price and both city-mpg (-0.695) and highway-mpg (-0.704). As the price of a car increases, its fuel efficiency in terms of miles per gallon (both in the city and on the highway) tends to decrease.\n",
        "Most Redundant Features:\n",
        "\n",
        "The two features that are likely to be most redundant are city-mpg and highway-mpg. They have a very strong positive correlation of approximately 0.974. This high positive correlation suggests that these two features provide very similar information. One of these features is likely sufficent\n",
        "Least Useful Feature for Predicting Price:\n",
        "\n",
        "Based on the correlation coefficients in the matrix, is peak-rpm. The correlation between peak-rpm and price is close to zero (-0.083). This indicates that there is little to no linear relationship between the engine's peak RPM and the price of the car. peak-rpm is unlikely to provide much predictive power for estimating the price of a car."
      ],
      "metadata": {
        "id": "J1fgmejNESH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow\n",
        "Let's train a linear regression model much like we did in the previous assignment, but this time using Tensorflow. Tensorflow is a powerful library with a complicated API so it is easy to get overwhelmed. We'll try to keep it simple.\n",
        "\n",
        "Build a model\n",
        "Here's how you use Tensorflow: First, you build a computational graph, and then you send data through it.\n",
        "\n",
        "This is confusing, but you'll get used to it. The computational graph for linear regression is very simple. There are many ways to build graphs, but the Keras library is recommended. Here, we're using keras.layers.Dense to create a model layer. We will go over Tensorflow and Keras in more detail, so don't worry about understanding everything now."
      ],
      "metadata": {
        "id": "8p3YKl4cEUVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(num_features, learning_rate):\n",
        "  \"\"\"Build a TF linear regression model using Keras.\n",
        "  Args:\n",
        "    num_features: The number of input features.\n",
        "    learning_rate: The desired learning rate for SGD.\n",
        "  Returns:\n",
        "    model: A tf.keras model (graph).\n",
        "  \"\"\"\n",
        "  # This is not strictly necessary, but each time you build a model, TF adds\n",
        "  # new nodes (rather than overwriting), so the colab session can end up\n",
        "  # storing lots of copies of the graph when you only care about the most\n",
        "  # recent. Also, as there is some randomness built into training with SGD,\n",
        "  # setting a random seed ensures that results are the same on each identical\n",
        "  # training run.\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  # Build a model using keras.Sequential. While this is intended for neural\n",
        "  # networks (which may have multiple layers), we want just a single layer for\n",
        "  # linear regression.\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim\n",
        "      input_shape=[num_features],  # input dim\n",
        "      use_bias=True,               # use a bias (intercept) param\n",
        "      kernel_initializer=tf.ones_initializer,  # initialize params to 1\n",
        "      bias_initializer=tf.ones_initializer,    # initialize bias to 1\n",
        "  ))\n",
        "\n",
        "  # We need to choose an optimizer. We'll use SGD, which is actually mini-batch\n",
        "  # SGD. We can specify the batch size to use for training later.\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "  # Finally, we compile the model. This finalizes the graph for training.\n",
        "  # We specify the MSE loss.\n",
        "  model.compile(loss='mse', optimizer=optimizer)\n",
        "  return model"
      ],
      "metadata": {
        "id": "dRe306B9EWv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we've built a model, we can inspect the initial parameters (weights). There should be two ( w0  and  b ) and they should be initialized to 1, which we specified above using tf.ones_initializer. Unlike our code in the last assignment, Tensorflow stores the bias/intercept separately from the other weights/parameters for the layer. We also don't need to prepend a column of 1s to learn the bias -- Tensorflow handles this for us."
      ],
      "metadata": {
        "id": "zctlFiR-EdRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model.\n",
        "model = build_model(num_features=1, learning_rate=0.0001)\n",
        "\n",
        "# Use get_weights() which returns lists of weights and biases for the layer.\n",
        "weights, biases = model.layers[0].get_weights()\n",
        "print('Weights:', weights)\n",
        "print('Biases:', biases)"
      ],
      "metadata": {
        "id": "QM8DAU-5EeCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model and look at the initial parameter values.\n",
        "model = build_model(num_features=2, learning_rate=0.0001)\n",
        "weights, biases = model.layers[0].get_weights()\n",
        "print('Weights:', weights)\n",
        "print('Biases:', biases)"
      ],
      "metadata": {
        "id": "sJPff94rEf0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait, we haven't trained yet! Why are we talking about making predictions? Well, remember that a model is a computational graph. That means we can pass data (of the expected shape) through the model (using the current values of the parameters) to make predictions.\n",
        "\n",
        "Before training, the parameters are set to their initial values (1s in our case). During training, we use the current predictions to compute a gradient and update the parameter values. Making predictions using the model without updating parameter values is called Inference.\n",
        "\n",
        "In the example code below, make sure you understand the output of predict."
      ],
      "metadata": {
        "id": "O3T7FHUMEh87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model that expects 1 input feature.\n",
        "model = build_model(num_features=1, learning_rate=0.0001)\n",
        "\n",
        "# Make a prediction for a single input.\n",
        "print(model.predict([42]))\n",
        "\n",
        "# Make predictions for 2 inputs.\n",
        "print(model.predict([42, 99]))"
      ],
      "metadata": {
        "id": "y_RJu2KgEjpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a model\n",
        "Now let's actually train a model, initially with just 1 feature -- the horsepower. Notice that the fit function can take pandas DataFrame objects for input (x) and output (y). In addition, we can convert the return value into a DataFrame that tracks training metrics (in this case, training data loss and validation data loss) after each epoch (a full pass through the training data).\n",
        "\n",
        "Remember that we're using SGD, which is actually mini-batch SGD. That is, each time the model estimates the loss for the current weights, it randomly samples a batch of training examples (of the specified size) to do so.\n",
        "\n",
        "Finally, we'll reserve some more examples (taken out of the training set) as a validation set. We use this data to check for overfitting while training. Why not use the test set for this purpose? We want to maintain the purity of the test set so we try to only use it at the end of the experimental process."
      ],
      "metadata": {
        "id": "mGRc5bZzElJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(num_features=1, learning_rate=0.000001)\n",
        "\n",
        "history = model.fit(\n",
        "  x = car_train_features[['horsepower']],\n",
        "  y = car_train_labels,\n",
        "  validation_split=0.1,  # use 10% of the examples as a validation set\n",
        "  epochs=5,\n",
        "  batch_size=32,\n",
        "  verbose=0)\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the loss after each\n",
        "# epoch. The history includes training data loss ('loss') and validation data\n",
        "# loss ('val_loss').\n",
        "history = pd.DataFrame(history.history)\n",
        "display(history)"
      ],
      "metadata": {
        "id": "0MFeiTv7EnIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling\n",
        "The loss is increasing as we train! What's going wrong?\n",
        "\n",
        "Look back at the histograms above. Notice that the scale of each feature value is different. Horsepower ranges from 48 to 262, while price ranges from $5118 to $45400. These different scales makes it more difficult to set the learning rate, and may make learning nearly impossible when we use multiple features (the scales of the gradients will overwhelm the actual feature importances).\n",
        "\n",
        "First, try reducing the learning rate above by 10x to 1e-5. That should fix the problem for now.\n",
        "\n",
        "But a better solution is to normalize the features so they are all roughly in the same range. We'll do this with mean and variance normalization. That is, for each feature, we subtract the mean (center the distribution on 0) and divide by the standard deviation (set the variance to 1)."
      ],
      "metadata": {
        "id": "7GiimAnGEpZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply mean and variance normalization to produce car_train_features_norm and car_test_features_norm. These should be copies of the car_train_features and car_test_features, but with normalized feature values. Note that we're not normalizing the labels (prices).\n",
        "\n",
        "DataFrame objects have mean and std functions you can use.\n",
        "Important: You can't normalize the test data by computing mean and variance on the test data, as this would violate our willful blindness of the test data.\n",
        "Use the describe function (as above) to verify your normalized data looks right."
      ],
      "metadata": {
        "id": "XyDTN1wHErjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean and std from the training data\n",
        "mean_train = car_train_features.mean()\n",
        "std_train = car_train_features.std()\n",
        "\n",
        "# Normalize the training features\n",
        "car_train_features_norm = (car_train_features - mean_train) / std_train\n",
        "\n",
        "# Normalize the test features using the mean and std from the training data\n",
        "car_test_features_norm = (car_test_features - mean_train) / std_train\n",
        "\n",
        "# Verify the normalized data using describe\n",
        "print(\"Normalized Training Data:\")\n",
        "print(car_train_features_norm.describe())\n",
        "print('\\n-------------------------------------------------------------')\n",
        "print(\"\\nNormalized Test Data:\")\n",
        "print(car_test_features_norm.describe())\n"
      ],
      "metadata": {
        "id": "v5-RMAEhEs83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with features\n",
        "We're ready to run some experiments with different sets of input features. To start, here's a simple function that plots train and validation set loss."
      ],
      "metadata": {
        "id": "C3yoHh0YEvTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(model, history):\n",
        "  \"\"\"Plot the loss after each training epoch.\"\"\"\n",
        "  # Convert the history object into a DataFrame.\n",
        "  history = pd.DataFrame(history.history)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(range(len(history)), history['loss'], marker='.', color='black')\n",
        "  plt.plot(range(len(history)), history['val_loss'], marker='.', color='red')\n",
        "  plt.legend(['train loss', 'validation loss'])\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.show()\n",
        "\n",
        "  # Show the final train loss value and the learned model weights.\n",
        "  print('Final train loss:', list(history['loss'])[-1])\n",
        "  print('Final weights:', model.layers[0].get_weights())"
      ],
      "metadata": {
        "id": "_Ao5AyNfEw-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrain the model predicting price from horsepower, but now using your normalized features. Report validation loss for learning rates [0.0001, 0.001, 0.01, 0.1, 1] after 150 epochs of training. Which produces the best validation loss? [Note: the function is currently only outputting training loss.]"
      ],
      "metadata": {
        "id": "5mPqtAgWEy0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of learning rates to test [0.0001, 0.001, 0.01, 0.1, 1]\n",
        "learning_rate = 1\n",
        "\n",
        "# Initialize a dictionary to store validation losses for each learning rate\n",
        "validation_losses = {}\n",
        "\n",
        "model = build_model(num_features=1, learning_rate=learning_rate)\n",
        "\n",
        "history = model.fit(\n",
        "  # use the normalized features prepared above\n",
        "  x = car_train_features_norm[['horsepower']],\n",
        "  y = car_train_labels,\n",
        "  validation_split=0.1,\n",
        "  epochs=150,\n",
        "  batch_size=32,\n",
        "  verbose=0)\n",
        "\n",
        "# Store the validation loss for this learning rate\n",
        "validation_loss = history.history['val_loss'][-1]\n",
        "validation_losses[learning_rate] = validation_loss\n",
        "\n",
        "plot_loss(model, history)\n",
        "print(f\"Learning Rate: {learning_rate}, Validation Loss: {validation_loss}\")"
      ],
      "metadata": {
        "id": "kFDsEGOYE0ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is to compile a table of results -- RMSE computed on the test data for the baseline and 4 models:\n",
        "\n",
        "features = horsepower\n",
        "features = horsepower, peak-rpm\n",
        "features = horsepower, peak-rpm, highway-mpg\n",
        "features = horsepower, peak-rpm, highway-mpg, city-mpg\n",
        "For consistency, use a batch size of 32, 150 epochs, and the best learning rate you found above."
      ],
      "metadata": {
        "id": "H0_zNfE1E3KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_learning_rate = 0.01 #add here the learning rate you found above\n",
        "\n",
        "features = ['horsepower']\n",
        "def run_experiment(features, learning_rate):\n",
        "  model = build_model(len(features), learning_rate)\n",
        "\n",
        "  history = model.fit(\n",
        "    x = car_train_features_norm[features],\n",
        "    y = car_train_labels,\n",
        "    validation_split=0.1,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    verbose=0)\n",
        "\n",
        "  plot_loss(model, history)\n",
        "\n",
        "  # Make predictions on test data\n",
        "  test_loss = model.evaluate(car_test_features_norm[features],\n",
        "                             car_test_labels,\n",
        "                             verbose=0)\n",
        "  test_rmse = np.sqrt(test_loss)\n",
        "  print('Test rmse:', test_rmse)\n",
        "\n",
        "run_experiment(features, best_learning_rate)"
      ],
      "metadata": {
        "id": "Zisr4WkAE44t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review\n",
        "The Pandas library is very useful for manipulating datasets and works well with numpy.\n",
        "Use a random split into train and test data and measure performance on the test data, starting from a simple baseline.\n",
        "Examine data using histograms and correlations to help build intuition before training any models.\n",
        "Tensorflow works by first building a computational graph; then, you can pass data through the graph to produce predictions, updating parameters via gradient descent in training mode; we use the Keras API to easily configure models.\n",
        "Training is often quite sensitive to the learning rate hyperparameter, and feature normalization is an important strategy to avoid differences in the scale of the feature derivatives (gradient) that can make learning impossible."
      ],
      "metadata": {
        "id": "v3jl7P4BE7Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the Fashion MNIST dataset.\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Flatten Y_train and Y_test, so they become vectors of label values.\n",
        "# The label for X_train[0] is in Y_train[0].\n",
        "Y_train = Y_train.flatten()\n",
        "Y_test = Y_test.flatten()"
      ],
      "metadata": {
        "id": "NaEqHYejFCFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"Y_train.shape:\", Y_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"Y_test.shape:\", Y_test.shape)"
      ],
      "metadata": {
        "id": "WuQQWs_yFFHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that there are 60,000 instances in X_train. Each of these is a grayscale image represented by an 28-by-28 array of grayscale pixel values between 0 and 255 (the larger the value, the lighter the pixel). Before we continue, let's apply linear scaling to our pixel values, so they all fall between 0 and 1."
      ],
      "metadata": {
        "id": "k9gkXGk2FHdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pixel values range from 0 to 255. To normalize the data,\n",
        "# we just need to divide all values by 255.\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ],
      "metadata": {
        "id": "hLB-oYQ3FIOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous lab, our input data had just a few features. Here, we treat every pixel value as a separate feature, so each input example has 28x28 (784) features!\n",
        "\n",
        "Fashion MNIST images have one of 10 possible labels (shown above). Since the labels are indices 0-9, let's keep a list of (string) names for convenience."
      ],
      "metadata": {
        "id": "RKNDh4fAFJn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "               'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']"
      ],
      "metadata": {
        "id": "Ys_YvDvdFLOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with subplots. This returns a list of object handles in axs\n",
        "# which we can use to populate the plots.\n",
        "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(10,5))\n",
        "for i in range(5):\n",
        "  image = X_train[i]\n",
        "  label = Y_train[i]\n",
        "  label_name = label_names[label]\n",
        "  axs[i].imshow(image, cmap='gray')\n",
        "  axs[i].set_title(label_name)\n",
        "  axs[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ax_GIUtZFNmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 10x5 grid for displaying the images --\n",
        "fig, axs = plt.subplots(nrows=10, ncols=5, figsize=(10, 15))\n",
        "for class_label in range(10):\n",
        "    class_indices = [i for i, label in enumerate(Y_train) if label == class_label][:5]\n",
        "    for i, image_index in enumerate(class_indices):\n",
        "        row = class_label\n",
        "        col = i\n",
        "        axs[row, col].imshow(X_train[image_index], cmap='gray')\n",
        "        axs[row, col].set_title(label_names[class_label])\n",
        "        axs[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l5ohFGCbFPp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sneaker Classification\n",
        "There are many things we can do with this dataset. Following our lectures, let's start with binary classification. We'll train a sneaker classifier, using sneaker images (class 7) as our positive examples (y=1) and all other images as negative examples (y=0).\n",
        "\n",
        "Once we've trained a model, it will produce predictions y^, the probability that an input image x is a sneaker.\n",
        "\n",
        "Data Preprocessing\n",
        "Before we continue, we need to prepare the data for our binary classification task. The label for all the sneaker images should be 1 and the label for all the non-sneaker images should be 0.\n",
        "\n",
        "Programming note: Numpy allows us to perform what is called boolean array indexing. This means that a numpy array  A  can be be modified according to boolean conditions in a matching sized array  B  as follows:  A[B] . We use this indexing to efficiently set our binary labels below."
      ],
      "metadata": {
        "id": "bfvKmNvtFS9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make copies of the original dataset for binary classification task.\n",
        "X_train_binary = np.copy(X_train)\n",
        "X_test_binary = np.copy(X_test)\n",
        "Y_train_binary = np.copy(Y_train)\n",
        "Y_test_binary = np.copy(Y_test)\n",
        "\n",
        "# Set labels: 1 for sneaker images, 0 for the others.\n",
        "# Note that a boolean array is created when Y_train_binary != 7 is evaluated.\n",
        "Y_train_binary[Y_train_binary != 7] = 0.0\n",
        "Y_train_binary[Y_train_binary == 7] = 1.0\n",
        "Y_test_binary[Y_test_binary != 7] = 0.0\n",
        "Y_test_binary[Y_test_binary == 7] = 1.0"
      ],
      "metadata": {
        "id": "ADlhW11DFTtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training any models, let's work some more with our raw feature values, comparing sneaker and non-sneaker images.\n",
        "\n",
        "Find all sneaker images in X_train_binary and compute the mean and standard deviation of the center pixel across all sneaker images. The center pixel is located at position [14, 14].\n",
        "\n",
        "Find all non-sneaker images in X_train_binary and compute the mean and standard deviation of the center pixel across all non-sneaker images. The center pixel is located at position [14, 14].\n",
        "\n",
        "Repeat 1 and 2 for the pixel located at positon [3, 14].\n",
        "\n",
        "Based on your results, do you think there is some evidence that suggests that we can use pixel values to discriminate between sneaker and non-sneaker images? Justify your answer."
      ],
      "metadata": {
        "id": "u1l--LhDFVwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "sneaker_images = X_train_binary[Y_train_binary == 1]  # Select sneaker images\n",
        "center_pixel_values_sneaker = sneaker_images[:, 14, 14]  # Extract center pixel values\n",
        "mean_center_pixel_sneaker = np.mean(center_pixel_values_sneaker)\n",
        "std_dev_center_pixel_sneaker = np.std(center_pixel_values_sneaker)\n",
        "\n",
        "#2\n",
        "non_sneaker_images = X_train_binary[Y_train_binary == 0]  # Select non-sneaker images\n",
        "center_pixel_values_non_sneaker = non_sneaker_images[:, 14, 14]  # Extract center pixel values\n",
        "mean_center_pixel_non_sneaker = np.mean(center_pixel_values_non_sneaker)\n",
        "std_dev_center_pixel_non_sneaker = np.std(center_pixel_values_non_sneaker)\n",
        "\n",
        "#3a\n",
        "pixel_sneaker_images = X_train_binary[Y_train_binary == 1]  # Select sneaker images\n",
        "pixal_center_pixel_values_sneaker = sneaker_images[:, 3, 14]  # Extract center pixel values\n",
        "pixal_mean_center_pixel_sneaker = np.mean(center_pixel_values_sneaker)\n",
        "pixal_std_dev_center_pixel_sneaker = np.std(center_pixel_values_sneaker)\n",
        "\n",
        "#3b\n",
        "pixal_non_sneaker_images = X_train_binary[Y_train_binary == 0]  # Select non-sneaker images\n",
        "pixal_center_pixel_values_non_sneaker = non_sneaker_images[:, 3, 14]  # Extract center pixel values\n",
        "pixal_mean_center_pixel_non_sneaker = np.mean(center_pixel_values_non_sneaker)\n",
        "pixal_std_dev_center_pixel_non_sneaker = np.std(center_pixel_values_non_sneaker)\n",
        "\n",
        "print(\"Center Pixel [14, 14] - Sneaker Images\")\n",
        "print(\"Mean:\", mean_center_pixel_sneaker)\n",
        "print(\"Standard Deviation:\", std_dev_center_pixel_sneaker)\n",
        "\n",
        "print(\"\\nCenter Pixel [14, 14] - Non-Sneaker Images\")\n",
        "print(\"Mean:\", mean_center_pixel_non_sneaker)\n",
        "print(\"Standard Deviation:\", std_dev_center_pixel_non_sneaker)\n",
        "\n",
        "\n",
        "print(\"\\nCenter Pixel [3, 14] - Sneaker Images\")\n",
        "print(\"Mean:\", pixal_mean_center_pixel_sneaker)\n",
        "print(\"Standard Deviation:\", pixal_std_dev_center_pixel_sneaker)\n",
        "\n",
        "\n",
        "print(\"\\nCenter Pixel [3, 14] - Non-Sneaker Images\")\n",
        "print(\"Mean:\", pixal_mean_center_pixel_non_sneaker)\n",
        "print(\"Standard Deviation:\", pixal_std_dev_center_pixel_non_sneaker)"
      ],
      "metadata": {
        "id": "aaGPMKpKFW0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Shuffling\n",
        "Just like in the previous lab, we'll be using SGD (Stochastic Gradient Descent) for training. This means that it is important that each batch is a random sample of the data.\n",
        "\n",
        "This time we can use integer array indexing to re-order the data and labels using a list of shuffled indices."
      ],
      "metadata": {
        "id": "KMVZzfOeGJw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0) # For reproducibility\n",
        "\n",
        "indices = np.arange(X_train_binary.shape[0])\n",
        "shuffled_indices = np.random.permutation(indices)\n",
        "\n",
        "# Re-order training examples and corresponding labels using the randomly\n",
        "# shuffled indices.\n",
        "X_train_binary = X_train_binary[shuffled_indices]\n",
        "Y_train_binary = Y_train_binary[shuffled_indices]"
      ],
      "metadata": {
        "id": "FkjrBCeuGKRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression\n",
        "Recall that logistic regression is an application of the logistic (sigmoid) function to the linear regression model:\n",
        "\n",
        "y=11+e−z\n",
        "\n",
        "Sigmoid Function\n",
        "where:\n",
        "\n",
        "z=b+w1x1+w2x2+...+wnxn\n",
        "\n",
        "As you can see, the output y is in the range [0,1], the probability of the positive class. If we want a label (instead of a probability), we need to choose a threshold, like 0.5. A value above or equal to that threshold indicates that the input should be classified as positive (e.g., sneaker); a value below the threshold indicates that the input should be classified as negative (e.g. non-sneaker).\n",
        "\n",
        "Loss Function\n",
        "In place of MSE, which we used for linear regression, we need a loss function for logistic regression that's appropriate for classification. The Log Loss (also known as binary cross-entropy), is defined as follows:\n",
        "\n",
        "−1|Y|∑yi∈Yyilog(y^i)+(1−yi)log(1−y^i)\n",
        "\n",
        "Recall that yi is the label for example i and yi^ is the predicted probability (of the positive class) for example i. Note that only the first term in the sum is active for positive examples (the second term is 0 when yi=1) and only the second term in the sum is active for negative examples (the first term is 0 when yi=0).\n",
        "\n",
        "The log loss is differentiable, allowing us to compute gradients and run SGD. It also happens to be convex, which guarantees that SGD (with a suitable learning rate) will produce a global minimum.\n",
        "\n",
        "Baseline\n",
        "When dealing with classification problems, a simple, but useful baseline is to select the majority class (the most common label in the training set) and use it as the prediction for all inputs.\n",
        "\n",
        "Our training dataset consists of 6,000 sneaker examples (10%), and 54,000 non-sneaker images (90%). So our majority class baseline classifies everything as non-sneaker. Notice that, for our particular dataset, this will yield an accuracy of 90%. Let's see if we can train a model that can beat the baseline."
      ],
      "metadata": {
        "id": "kf8W2AIuGL7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of sneaker images in training set: %d\"%(Y_train_binary == 1).sum())\n",
        "print(\"Number of non-sneaker images in training set: %d\"%(Y_train_binary == 0).sum())"
      ],
      "metadata": {
        "id": "vfPierjUGPMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a function that computes the Log Loss (binary cross-entropy) metric and use it to evaluate our baseline on both the train and test data. Use 0.1 as the predicted probability for your baseline (reflecting what we know about the original distribution of classes in our dataset)."
      ],
      "metadata": {
        "id": "-z-zqlt_GSy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(Y_true, Y_pred):\n",
        "  \"\"\"Returns the binary log loss for a list of labels and predictions.\n",
        "\n",
        "  Args:\n",
        "    Y_true: A list of (true) labels (0 or 1)\n",
        "    Y_pred: A list of corresponding predicted probabilities\n",
        "\n",
        "  Returns:\n",
        "    Binary log loss\n",
        "  \"\"\"\n",
        "  epsilon = 1e-15  # Small constant to prevent division by zero\n",
        "  Y_pred = np.clip(Y_pred, epsilon, 1 - epsilon)  # Clip predicted probabilities to avoid extreme values\n",
        "  log_loss = - (Y_true * np.log(Y_pred) + (1 - Y_true) * np.log(1 - Y_pred))\n",
        "  return np.mean(log_loss)\n",
        "\n",
        "# Baseline predicted probability (given)\n",
        "baseline_prob = 0.1\n",
        "\n",
        "# Create arrays of labels for the train and test data\n",
        "train_predicted_probabilities = np.full_like(Y_train_binary, baseline_prob)\n",
        "test_predicted_probabilities = np.full_like(Y_test_binary, baseline_prob)\n",
        "\n",
        "# Calculate log loss for train and test data\n",
        "train_log_loss = log_loss(Y_train_binary, train_predicted_probabilities)\n",
        "test_log_loss = log_loss(Y_test_binary, test_predicted_probabilities)\n",
        "\n",
        "print(\"Log Loss on Train Data (Baseline):\", train_log_loss)\n",
        "print(\"Log Loss on Test Data (Baseline):\", test_log_loss)"
      ],
      "metadata": {
        "id": "XqcYyKN-GRCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a model\n",
        "We will use Tensorflow/Keras to build our logistic regression model. This should look very similar to the models you built for linear regression but with a few key differences:\n",
        "\n",
        "We use the Keras flatten layer to turn the 2-D 28x28 pixel grid inputs into 1-D vector inputs.\n",
        "We configure our dense layer with a sigmoid activation, which applies a sigmoid function to the output of the linear mapping  xWT .\n",
        "We specify binary_crossentropy as the loss (synonymous with log loss) when compiling the model."
      ],
      "metadata": {
        "id": "msDodk4kGWep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(learning_rate=0.01):\n",
        "  \"\"\"Build a TF logistic regression model using Keras.\n",
        "\n",
        "  Args:\n",
        "    learning_rate: The desired learning rate for SGD.\n",
        "\n",
        "  Returns:\n",
        "    model: A tf.keras model (graph).\n",
        "  \"\"\"\n",
        "  # This is not strictly necessary, but each time you build a model, TF adds\n",
        "  # new nodes (rather than overwriting), so the colab session can end up\n",
        "  # storing lots of copies of the graph when you only care about the most\n",
        "  # recent. Also, as there is some randomness built into training with SGD,\n",
        "  # setting a random seed ensures that results are the same on each identical\n",
        "  # training run.\n",
        "  tf.keras.backend.clear_session()\n",
        "  np.random.seed(0)\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  # Build a model using keras.Sequential.\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  # Keras layers can do pre-processing. This layer will take our 28x28 images\n",
        "  # and flatten them into vectors of size 784.\n",
        "  model.add(keras.layers.Flatten())\n",
        "\n",
        "  # This layer constructs the linear set of parameters for each input feature\n",
        "  # (as well as a bias), and applies a sigmoid to the result. The result is\n",
        "  # binary logistic regression.\n",
        "  model.add(keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      use_bias=True,               # use a bias param\n",
        "      activation=\"sigmoid\"         # apply the sigmoid function!\n",
        "  ))\n",
        "\n",
        "  # Use the SGD optimizer as usual.\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "  # We specify the binary_crossentropy loss (equivalent to log loss).\n",
        "  # Notice that we are including 'binary accuracy' as one of the metrics that we\n",
        "  # ask Tensorflow to report when evaluating the model.\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=optimizer,\n",
        "                metrics=[metrics.binary_accuracy])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "y0_bYlhNGZBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure model building code works. Before training, the parameters of the model are initialized randomly (this is the default). While the untrained model won't make good predictions, we should still be able to pass data through it and get probability outputs."
      ],
      "metadata": {
        "id": "niRP5sKNGclG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model.\n",
        "model = build_model()\n",
        "\n",
        "# Make a prediction for five inputs.\n",
        "print(model.predict(X_train_binary[0:5]))"
      ],
      "metadata": {
        "id": "YXQ7rddFGe38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the outputs look like probabilities (in [0,1]). Once the model is trained, we hope that these predictions correspond to the probability that each input image is a sneaker."
      ],
      "metadata": {
        "id": "C8Kw7SjVGg7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the model. Note that we're using 10% of the training data as a validation split. This serves a similar purpose to our test data, allowing us to check for over-fitting during training. We don't use the test data here because we might run lots of experiments, and over time, we might adjust settings to improve results on the validation set. We want to preserve the purity of the test data to allow for the cleanest possible evaluation at the end of the experimentation process."
      ],
      "metadata": {
        "id": "E6EUyhCuGi09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(learning_rate=0.01)\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train_binary,   # our binary training examples\n",
        "  y = Y_train_binary,   # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size for SGD\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss\n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "display(history)"
      ],
      "metadata": {
        "id": "_DRCFX0XGkLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good news. It appears that our model is doing better than our baseline. Let's use the trained model to predict probabilities for the test data. We can use predict to run inference."
      ],
      "metadata": {
        "id": "elJUVxMSGmUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The result of model.predict has an extra dimension, so we flatten to get a\n",
        "# vector of predictions. Note that these are the predicted probabilities of the\n",
        "# positive (sneaker) class.\n",
        "test_predictions = model.predict(X_test_binary).flatten()\n",
        "print(test_predictions.shape)\n",
        "print(test_predictions)"
      ],
      "metadata": {
        "id": "HI3OXa8UGnrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the accuracy on the test data using a threshold of 0.5. Remember to use Y_test_binary for the true labels."
      ],
      "metadata": {
        "id": "P__jqGDAGqZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_log_loss(Y_true, Y_pred):\n",
        "    epsilon = 1e-15\n",
        "    Y_pred = np.clip(Y_pred, epsilon, 1 - epsilon)\n",
        "    log_loss = - (Y_true * np.log(Y_pred) + (1 - Y_true) * np.log(1 - Y_pred))\n",
        "    return np.mean(log_loss)\n",
        "\n",
        "\n",
        "# Threshold for binary classification\n",
        "threshold = 0.5\n",
        "\n",
        "# Create arrays of predicted probabilities using the baseline probability (0.1)\n",
        "test_predicted_probabilities = np.full_like(Y_test_binary, 0.1)\n",
        "\n",
        "# Calculate log loss using the test_log_loss function\n",
        "test_log_loss_value = test_log_loss(Y_test_binary, test_predicted_probabilities)\n",
        "\n",
        "# Convert predicted probabilities to binary predictions based on the threshold (0.5)\n",
        "test_binary_predictions = (test_predicted_probabilities >= threshold).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = (test_binary_predictions == Y_test_binary).sum()\n",
        "total_predictions = len(Y_test_binary)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(\"Log Loss on Test Data (Baseline):\", test_log_loss_value)\n",
        "print(\"Accuracy on Test Data:\", accuracy)\n"
      ],
      "metadata": {
        "id": "yU4FurvVGrtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze the model\n",
        "Let's investigate what the model has learned. Recall how to get the learned weights from the model:"
      ],
      "metadata": {
        "id": "r_fkkY2zGtbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model includes 2 layers: a flattening layer and a dense layer.\n",
        "print(model.layers)\n",
        "\n",
        "# Retrieve the weights and biases from the dense layer.\n",
        "weights, biases = model.layers[1].get_weights()\n",
        "bias = biases[0]  # there's only 1 bias\n",
        "weights = weights.flatten()  # flatten the weights to a vector\n",
        "print('Bias:', bias)\n",
        "print('Weights shape:', weights.shape)"
      ],
      "metadata": {
        "id": "9miCJEOtGvsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using just the bias calculated in the model, compute the predicted probability (of the sneaker class) for an image of all 0-valued inputs.\n",
        "\n",
        "Construct a fake image with all 0s and use model.predict.\n",
        "\n",
        "Confirm if they agree"
      ],
      "metadata": {
        "id": "5Po5pG_ZGxky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Bias:', bias)\n",
        "\n",
        "# Calculate the predicted probability using just the bias no image\n",
        "predicted_probability = 1 / (1 + np.exp(-bias))\n",
        "\n",
        "print('Predicted Probability (Sneaker Class) for All 0-valued Inputs:', predicted_probability)\n",
        "\n",
        "\n",
        "#Construct a fake image with all 0s (shape: (1, 28, 28))\n",
        "fake_image = np.zeros((1, 28, 28))\n",
        "\n",
        "# Use the model to get the predicted probability for the fake image\n",
        "predicted_probability = model.predict(fake_image)\n",
        "print('Predicted Probability (Sneaker Class) for Fake Image:', predicted_probability[0])\n"
      ],
      "metadata": {
        "id": "cmE5ygFgGyG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This lab extends binary logistic regression to multi-class logistic regression, which goes by a variety of names, including softmax regression, due to the use of the softmax function, which generalizes the logistic function."
      ],
      "metadata": {
        "id": "o8ToIsI8HA-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the Fashion MNIST dataset.\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# Flatten Y_train and Y_test, so they become vectors of label values.\n",
        "# The label for X_train[0] is in Y_train[0].\n",
        "Y_train = Y_train.flatten()\n",
        "Y_test = Y_test.flatten()\n",
        "\n",
        "label_names = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "               'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "\n",
        "# Apply random shufflying to training examples.\n",
        "np.random.seed(0)\n",
        "indices = np.arange(X_train.shape[0])\n",
        "shuffled_indices = np.random.permutation(indices)\n",
        "X_train = X_train[shuffled_indices]\n",
        "Y_train = Y_train[shuffled_indices]"
      ],
      "metadata": {
        "id": "IjJWUrLoHCdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Entropy Loss\n",
        "Recall the log loss function (also called binary cross-entropy):\n",
        "\n",
        "−ylog(y^)+(1−y)log(1−y^)\n",
        "\n",
        "In the above formulation, it is assumed that y is either 0 or 1, so either the left term or the right term is active for each example.\n",
        "\n",
        "The general form for cross-entropy is used when y is assumed to be a label vector with a 1 in the index of the true label and a 0 everywhere else: y=[0,0,0,0,0,0,0,1,0,0] implies a label of \"sneaker\" in this dataset (the 7th label). Accordingly, y^ is a vector of predicted probabilities. Then the cross-entropy loss is simply:\n",
        "\n",
        "−∑jyjlog(y^j)\n",
        "\n",
        "As in the binary case, this summation will have exactly 1 non-zero term where the true label yj=1.\n",
        "\n",
        "Note that this formulation is using a dense representation of the label. The corresponding sparse representation would use the non-zero index directly (y=7).\n",
        "\n",
        "Build a model\n",
        "Let's construct a model much like we did in the binary classification case, but now with a multi-class output."
      ],
      "metadata": {
        "id": "WXnSGoz_HEvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the NotImplemented parts of the build_model function below. You will need to make the following changes to generalize the binary case to the multi-class case:\n",
        "\n",
        "The output will include n_classes probabilities instead of 1.\n",
        "Use a softmax function instead of a sigmoid.\n",
        "Use a sparse_categorical_crossentropy loss instead of binary_crossentropy. Note that \"sparse\" refers to the use of a sparse index (e.g. 7) to indicate the label rather than a dense vector (e.g. [0,0,0,0,0,0,0,1,0,0]).\n",
        "Check that training works below."
      ],
      "metadata": {
        "id": "nJDlYWDVHIrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(n_classes, learning_rate=0.01):\n",
        "  \"\"\"Build a multi-class logistic regression model using Keras.\n",
        "\n",
        "  Args:\n",
        "    n_classes: Number of classes in the dataset\n",
        "    learning_rate: The desired learning rate for SGD.\n",
        "\n",
        "  Returns:\n",
        "    model: A tf.keras model (graph).\n",
        "  \"\"\"\n",
        "  tf.keras.backend.clear_session()\n",
        "  np.random.seed(0)\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dense(\n",
        "      # YOUR CODE HERE\n",
        "      units= n_classes,\n",
        "      activation='softmax'\n",
        "  ))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  model.compile(loss='sparse_categorical_crossentropy',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "mrcHwiDiHHjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(len(label_names), 0.01)\n",
        "\n",
        "history = model.fit(\n",
        "  x = X_train,\n",
        "  y = Y_train,\n",
        "  epochs=5,\n",
        "  batch_size=64,\n",
        "  validation_split=0.1,\n",
        "  verbose=1)\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "display(history)"
      ],
      "metadata": {
        "id": "2iWb31qQHNwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics\n",
        "Classification metrics like accuracy, precision, and recall can all be derived from a confusion matrix which displays the counts for all pairs of true label and predicted label. Correct predictions are on the diagonal and incorrect predictions (confusions) are off the diagonal.\n",
        "\n",
        "First, we need the predicted labels from the model."
      ],
      "metadata": {
        "id": "8UkzTuHsHPnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall that model.predict gives a vector of probabilities for each x.\n",
        "# Get labels by taking the argmax -- the index with the largest probability.\n",
        "test_predictions = np.argmax(model.predict(X_test), axis=-1)\n",
        "print(test_predictions)"
      ],
      "metadata": {
        "id": "2zRO44p8HRYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix as a 2D array.\n",
        "confusion_matrix = tf.math.confusion_matrix(Y_test, test_predictions)\n",
        "\n",
        "# Use a heatmap plot to display it.\n",
        "ax = sns.heatmap(confusion_matrix, annot=True, fmt='.3g', cmap='Blues',\n",
        "                 xticklabels=label_names, yticklabels=label_names, cbar=False)\n",
        "\n",
        "# Add axis labels.\n",
        "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
        "plt.show()\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "id": "WCOovW4rHTAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze Errors\n",
        "A crucial part of the development cycle in Machine Learning is analyzing errors to help understand the shortcomings of the model. While typically we'd want to use the development data for this purpose to preserve the purity of the test set, we'll just use our test split for simplicity.\n",
        "\n",
        "Since the \"shirt\" class seems to be the source for a lot of errors, let's look at some of the confusions.\n",
        "\n",
        "Exercise 3 (33 points)\n",
        "Display 5 images with true label \"shirt\", but predicted label \"coat\" (false negatives for the \"shirt\" class).\n",
        "Display 5 images with predicted label \"shirt\", but true label \"coat\" (false positives for the \"shirt\" class)."
      ],
      "metadata": {
        "id": "DtKflEOeHU4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to display images\n",
        "def display_images(images, true_labels, test_predictions, title):\n",
        "    plt.figure(figsize=(18, 4))\n",
        "    for i in range(5):\n",
        "        plt.subplot(1, 5, i + 1)\n",
        "        plt.imshow(images[i], cmap='gray')\n",
        "        plt.title(f'True: {true_labels[i]}\\nPredicted: {test_predictions[i]}')\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "# Find indices of false negatives (true label \"shirt\" but predicted label \"coat\")\n",
        "false_negatives_indices = np.where((Y_test == label_names.index(\"shirt\")) & (test_predictions == label_names.index(\"coat\")))[0]\n",
        "\n",
        "# Find indices of false positives (true label \"coat\" but predicted label \"shirt\")\n",
        "false_positives_indices = np.where((Y_test == label_names.index(\"coat\")) & (test_predictions == label_names.index(\"shirt\")))[0]\n",
        "\n",
        "# Randomly select 5 examples from each category\n",
        "np.random.seed(0)\n",
        "false_negatives_samples = np.random.choice(false_negatives_indices, 5, replace=False)\n",
        "false_positives_samples = np.random.choice(false_positives_indices, 5, replace=False)\n",
        "\n",
        "# Display false negatives\n",
        "display_images(X_test[false_negatives_samples], Y_test[false_negatives_samples], test_predictions[false_negatives_samples], \"False Negatives (True: 'shirt', Predicted: 'coat')\")\n",
        "\n",
        "# Display false positives\n",
        "display_images(X_test[false_positives_samples], Y_test[false_positives_samples],test_predictions[false_positives_samples], \"False Positives (True: 'coat', Predicted: 'shirt')\")"
      ],
      "metadata": {
        "id": "oQmg5arSHWqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, here's some code that helps visualize the learned parameters for each class."
      ],
      "metadata": {
        "id": "ze1UPGwNHZFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = model.layers[1].get_weights()\n",
        "fig, axs = plt.subplots(2, 5, figsize=(16,7))\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "  ax.imshow(weights[:,i].reshape(28,28), cmap='PRGn')\n",
        "  ax.axis('off')\n",
        "  ax.set_title(label_names[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1PJw9ZJ3HajU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}